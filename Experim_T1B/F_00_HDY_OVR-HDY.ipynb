{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## <font color=\"#CA3532\"> Cuantificación </font>\n",
    "\n",
    "##### <font color=\"#CA7868\"> TFM datos reales </font>\n",
    "##### <font color=\"#CA7868\"> HDY HDY_OVR </font>\n",
    "##### <font color=\"#CA7868\"> Fuente de datos: https://zenodo.org/record/6546188#.Yp28_DlByHs </font>\n",
    "###### <font color=\"#CA7868\"> Este data set contiene un total de 28 clases compuesto por 20.000 ejemplos de entrenamiento cada uno descrito por 300 caracteristicas además de 5000 muestras de test cada una con 1000 ejemplos </font>\n",
    "##### <font color=\"#CA7868\"> Abrán Yiu-sen Yuen Durán</font>\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "#### <font color=\"#CA3532\"> Paquetes Necesarios </font>\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from quantificationlib.multiclass.df import HDy,HDX,DFX\n",
    "from quantificationlib.decomposition.multiclass import OneVsRestQuantifier  # --> NECESARIO PARA CUANTIFICAR\n",
    "\n",
    "\n",
    "from quantificationlib.estimators.cross_validation import CV_estimator\n",
    "\n",
    "\n",
    "def absolute_error(prevs, prevs_hat):\n",
    "    assert prevs.shape == prevs_hat.shape, 'wrong shape {prevs.shape} vs. {prevs_hat.shape}'\n",
    "    return abs(prevs_hat - prevs).mean(axis=-1)\n",
    "\n",
    "\n",
    "def relative_absolute_error(p, p_hat, eps=None):\n",
    "    def __smooth(prevs, epsilon):\n",
    "        n_classes = prevs.shape[-1]\n",
    "        return (prevs + epsilon) / (epsilon * n_classes + 1)\n",
    "\n",
    "    p = __smooth(p, eps)\n",
    "    p_hat = __smooth(p_hat, eps)\n",
    "    return (abs(p-p_hat)/p).mean(axis=-1)\n",
    "\n",
    "\n",
    "def load_training_set(dfile):\n",
    "    data = np.genfromtxt(dfile, skip_header=1, delimiter=',')\n",
    "\n",
    "    X = data[:, 1:]\n",
    "    y = data[:, 0].astype(int)\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def load_testing_bag(dfile):\n",
    "    X = np.genfromtxt(dfile, skip_header=1, delimiter=',')\n",
    "    return X\n",
    "\n",
    "\n",
    "def load_prevalences(dfile):\n",
    "    data = np.genfromtxt(dfile, skip_header=1, delimiter=',')\n",
    "\n",
    "    prevalences = data[:, 1:]\n",
    "    return prevalences\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path  \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "#### <font color=\"#CA3532\"> Propensiones por Método HDY , OVR-HDY</font>\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(path, dataset, estimator_name, n_bags=100, bag_inicial=0, master_seed=2032):\n",
    "\n",
    "    # <<<>>>> =================================================================================== <<<>>>> \n",
    "    X_train, y_train = load_training_set(path + dataset + '/public/training_data.txt')\n",
    "\n",
    "    # Clasificadores disponibles\n",
    "    # ===================================================================================================\n",
    "    if estimator_name == 'LR':\n",
    "        skf_train = StratifiedKFold(n_splits=10, shuffle=True, random_state=master_seed)\n",
    "        estimator = LogisticRegression(C=0.01, max_iter=1000, class_weight='balanced')\n",
    "        estimator_train = CV_estimator(estimator=estimator, cv=skf_train)\n",
    "        estimator_test = None \n",
    "    elif estimator_name == 'CLLR':\n",
    "        skf_train = StratifiedKFold(n_splits=10, shuffle=True, random_state=master_seed)\n",
    "        estimator = CalibratedClassifierCV(LogisticRegression(C=0.01, max_iter=1000, class_weight='balanced'))\n",
    "        estimator_train = CV_estimator(estimator=estimator, cv=skf_train)\n",
    "        estimator_test = None \n",
    "    else:\n",
    "        raise ValueError('Unknwon estimator')\n",
    "\n",
    "    print('Fitting Training Estimator')\n",
    "    estimator_train.fit(X_train, y_train)\n",
    "    print('Training Estimator fitted', flush=True)\n",
    "    probs_train = estimator_train.predict_proba(X_train)\n",
    "    print('Prediction_train computed')\n",
    "    \n",
    "    # Méthods\n",
    "    # Multivariantes\n",
    "    # ===================================================================================================\n",
    "    # ===================================================================================================\n",
    "    # HDY\n",
    "    hdy = HDy(n_bins=4, bin_strategy='equal_width')\n",
    "    hdy.fit(X_train, y_train, predictions_train=probs_train)\n",
    "    print('HDY fitted')\n",
    "        \n",
    "    # One vs Rest\n",
    "    # ===================================================================================================\n",
    "    # ===================================================================================================\n",
    "    # En algunos casos se precisa de un estimador One versus rest, sobre todo para las Y\n",
    "    ovr_estimator = OneVsRestClassifier(estimator, n_jobs=-1)  \n",
    "    # ovr_HDY \n",
    "    ovr_hdy = OneVsRestQuantifier(base_quantifier=HDy(n_bins=4, bin_strategy='equal_width'),\n",
    "                                  estimator_train=ovr_estimator, estimator_test=ovr_estimator) # Necesita estimador OVR\n",
    "    ovr_hdy.fit(X_train, y_train)\n",
    "    print('ovr_hdy fitted')\n",
    "    \n",
    "    \n",
    "    # Definicion de Estimador según caso\n",
    "    # ===================================================================================================\n",
    "    print('Fitting Estimator Test')\n",
    "    if estimator_test is None:\n",
    "        estimator_test = estimator_train\n",
    "    else:\n",
    "        estimator_test.fit(X_train, y_train)\n",
    "    print('Estimator test fitted')\n",
    "    \n",
    "    print('Fitting ovr_estimator Test')\n",
    "    if ovr_estimator is None:\n",
    "        ovr_estimator = estimator_train\n",
    "    else:\n",
    "        ovr_estimator.fit(X_train, y_train)\n",
    "    print('ovr_estimator test fitted')\n",
    "\n",
    "    # Carga de prevalencias reales\n",
    "    # <<<>>>> =================================================================================== <<<>>>>  \n",
    "    prev_true = load_prevalences(path + dataset + '/public/test_prevalences.txt')\n",
    "    \n",
    "    \n",
    "    # Export de prevalencias según método\n",
    "    # ===================================================================================================\n",
    "    df_export=[]\n",
    "    df_export_ovr=[]\n",
    "    for n_bag in range(n_bags):\n",
    "        # <<<>>>> =================================================================================== <<<>>>>  \n",
    "        X_test = load_testing_bag(path + dataset + '/public/test_samples/' + str(n_bag) + '.txt')\n",
    "        \n",
    "        probs_test = estimator_test.predict_proba(X_test)\n",
    "        probs_test_ovr = ovr_estimator.predict_proba(X_test)  \n",
    "        \n",
    "        # EXPORT HDY\n",
    "        prev_preds_HDY = [hdy.predict(X=None, predictions_test=probs_test)]\n",
    "        true_prev=prev_true[bag_inicial + n_bag, :]\n",
    "        df_real = pd.DataFrame(true_prev,columns=['REAL'])\n",
    "        df_prev = pd.DataFrame(prev_preds_HDY[0],columns=['PREDICHAS']) \n",
    "        df_prev['BAG']=n_bag\n",
    "        df_prev['METODO']='HDY'   \n",
    "        df=df_real.join(df_prev)\n",
    "        # append para exportar\n",
    "        df=df.append(df_export)\n",
    "        df_export=df\n",
    "        \n",
    "        # EXPORT OVR-HDY\n",
    "        prev_preds_OVR_HDY = [ovr_hdy.predict(X=None, predictions_test=probs_test_ovr)] \n",
    "        true_prev=prev_true[bag_inicial + n_bag, :]\n",
    "        df_real = pd.DataFrame(true_prev,columns=['REAL'])\n",
    "        df_prev = pd.DataFrame(prev_preds_OVR_HDY[0],columns=['PREDICHAS']) \n",
    "        df_prev['BAG']=n_bag\n",
    "        df_prev['METODO']='OVR-HDY'  \n",
    "        df=df_real.join(df_prev)\n",
    "        # append para exportar\n",
    "        df=df.append(df_export_ovr)\n",
    "        df_export_ovr=df\n",
    "        \n",
    "                        \n",
    "    # <<<>>>> =================================================================================== <<<>>>>      \n",
    "    filepath = Path('/Users/abran.yuen/00_FINAL_TFM/00_Real_Data_Export/HDY-'+str(estimator_name)+'-n_bags-'+str(n_bag+1)+'.csv')  \n",
    "                    \n",
    "    filepath.parent.mkdir(parents=True, exist_ok=True)  \n",
    "    df_export.to_csv(filepath) \n",
    "    \n",
    "    filepath = Path('/Users/abran.yuen/00_FINAL_TFM/00_Real_Data_Export/OVR_HDY-'+str(estimator_name)+'-n_bags-'+str(n_bag+1)+'.csv')  \n",
    "                    \n",
    "    filepath.parent.mkdir(parents=True, exist_ok=True)  \n",
    "    df_export_ovr.to_csv(filepath) \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting Training Estimator\n",
      "Training Estimator fitted\n",
      "Prediction_train computed\n",
      "HDY fitted\n",
      "ovr_hdy fitted\n",
      "Fitting Estimator Test\n",
      "Estimator test fitted\n",
      "Fitting ovr_estimator Test\n",
      "ovr_estimator test fitted\n"
     ]
    }
   ],
   "source": [
    "main(path='/Users/abran.yuen/Desktop/tfm_dataset/', dataset='T1B',  estimator_name='CLLR', n_bags=100, bag_inicial=0, master_seed=2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting Training Estimator\n",
      "Training Estimator fitted\n",
      "Prediction_train computed\n",
      "HDY fitted\n",
      "ovr_hdy fitted\n",
      "Fitting Estimator Test\n",
      "Estimator test fitted\n",
      "Fitting ovr_estimator Test\n",
      "ovr_estimator test fitted\n"
     ]
    }
   ],
   "source": [
    "main(path='/Users/abran.yuen/Desktop/tfm_dataset/', dataset='T1B',  estimator_name='LR', n_bags=100, bag_inicial=0, master_seed=2022)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"#CA7868\"> MAE </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# MAE\n",
    "def absolute_error(prevs, prevs_hat):\n",
    "    #assert prevs.shape == prevs_hat.shape, 'wrong shape {prevs.shape} vs. {prevs_hat.shape}'\n",
    "    return abs(prevs_hat - prevs).mean(axis=-1)\n",
    "\n",
    "# RMSE\n",
    "def relative_absolute_error(p, p_hat, eps=None):\n",
    "    def __smooth(prevs, epsilon):\n",
    "        n_classes = prevs.shape[-1]\n",
    "        return (prevs + epsilon) / (epsilon * n_classes + 1)\n",
    "\n",
    "    p = __smooth(p, eps)\n",
    "    p_hat = __smooth(p_hat, eps)\n",
    "    return (abs(p-p_hat)/p).mean(axis=-1)\n",
    "\n",
    "# Preprocesado para calcular errores\n",
    "def prepro(df):\n",
    "    REALES=df['REAL'].to_numpy()\n",
    "    PREDICHAS=df['PREDICHAS'].to_numpy()\n",
    "    return REALES,PREDICHAS\n",
    "\n",
    "HDY=pd.read_csv('/Users/abran.yuen/00_FINAL_TFM/00_Real_Data_Export/HDY-LR-n_bags-100.csv')  \n",
    "OVR_HDY=pd.read_csv('/Users/abran.yuen/00_FINAL_TFM/00_Real_Data_Export/OVR_HDY-LR-n_bags-100.csv')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HDY  y OVR_HDY   tienen misma dimensión\n"
     ]
    }
   ],
   "source": [
    "if HDY.shape != OVR_HDY.shape:\n",
    "    print('HDY y OVR_HDY necesita preprocesado')\n",
    "else:\n",
    "    print('HDY  y OVR_HDY   tienen misma dimensión')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=\"#CA7868\"> MAE para LR </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================= MAE =================\n",
      "HDY      0.011954580872043074\n",
      "OVR_HDY  0.038988841514951696\n",
      " \n"
     ]
    }
   ],
   "source": [
    "REALES_HDY=prepro(HDY)[0]\n",
    "PREDICHAS_HDY=prepro(HDY)[1]\n",
    "REALES_OVR_HDY=prepro(OVR_HDY)[0]\n",
    "PREDICHAS_OVR_HDY=prepro(OVR_HDY)[1]\n",
    "print('================= MAE =================')\n",
    "print('HDY     ',absolute_error(REALES_HDY,PREDICHAS_HDY))\n",
    "print('OVR_HDY ',absolute_error(REALES_OVR_HDY,PREDICHAS_OVR_HDY))\n",
    "print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================= RMSE =================\n",
      "HDY      1.0388820127215572\n",
      "OVR_HDY  5.5590625970584275\n",
      " \n"
     ]
    }
   ],
   "source": [
    "print('================= RMSE =================')\n",
    "print('HDY     ',relative_absolute_error(REALES_HDY,PREDICHAS_HDY,eps=0.0005))\n",
    "print('OVR_HDY ',relative_absolute_error(REALES_OVR_HDY,PREDICHAS_OVR_HDY,eps=0.0005))\n",
    "print(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=\"#CA7868\"> MAE para CLLR</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "HDY=pd.read_csv('/Users/abran.yuen/00_FINAL_TFM/00_Real_Data_Export/HDY-CLLR-n_bags-100.csv')  \n",
    "OVR_HDY=pd.read_csv('/Users/abran.yuen/00_FINAL_TFM/00_Real_Data_Export/OVR_HDY-CLLR-n_bags-100.csv')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================= MAE =================\n",
      "HDY      0.017777129827112884\n",
      "OVR_HDY  0.035214709634294114\n",
      " \n"
     ]
    }
   ],
   "source": [
    "REALES_HDY=prepro(HDY)[0]\n",
    "PREDICHAS_HDY=prepro(HDY)[1]\n",
    "REALES_OVR_HDY=prepro(OVR_HDY)[0]\n",
    "PREDICHAS_OVR_HDY=prepro(OVR_HDY)[1]\n",
    "print('================= MAE =================')\n",
    "print('HDY     ',absolute_error(REALES_HDY,PREDICHAS_HDY))\n",
    "print('OVR_HDY ',absolute_error(REALES_OVR_HDY,PREDICHAS_OVR_HDY))\n",
    "print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================= RMSE =================\n",
      "HDY      1.7851032188053568\n",
      "OVR_HDY  4.077302698877278\n",
      " \n"
     ]
    }
   ],
   "source": [
    "print('================= RMSE =================')\n",
    "print('HDY     ',relative_absolute_error(REALES_HDY,PREDICHAS_HDY,eps=0.0005))\n",
    "print('OVR_HDY ',relative_absolute_error(REALES_OVR_HDY,PREDICHAS_OVR_HDY,eps=0.0005))\n",
    "print(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#FIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
